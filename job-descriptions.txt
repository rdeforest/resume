AWS

I co-founded the DDoS response team. The team served as live 24/7 support for DDoS activity as it happened. Before this team existed, responses were ad-hoc and disorganized. We also provided internal customers with reports about recent DDoS activity. I scaled out the existing DDoS detection and mitigation solution to four new regions. I implemented automation which reduced by a factor of three the effort to scale out to new regions. During a period when my team was at risk of missing a deadline I volunteered to cover all daytime on-call for seven weeks in a row, freeing up engineers to concentrate on the development effort to guarantee timely delivery of a product.

My primary duty was to serve as development support to the internal mobile payments project (later canceled). These duties included a bake-off of competing DUKPT Hardware Security Module products, verifying advertised performance and features. I produced a report describing my experience working with the companies' support teams, using their documentation and working with the equipment, as well as providing a recommendation, which was in fact adopted. I also led a project to help a third party vendor deploy their product to AWS.

Secondary to that duty, I worked as on-call for the Payments Systems Operations team. We were responsible for the security and correct/timely functioning of the payments infrastructure. I worked with payment processors to troubleshoot issues sending and receiving transactions, including (for example) an error due to one processor not thinking they would ever see a certain number of transactions in the same day.

I also was the primary owner of our AS2 connectivity solution. The solution existed when I joined the team, so I had to get up to speed on it, then having done so, write documentation and train the rest of our team.

My team also consulted with internal development teams to investigate reliability problems to reduce on-call work.

AMAZON PAYMENTS

I was responsible for the correct, private, reliable operation of the data management system at SCHARP. The organization had hundreds of clinical trials field offices (or 'sites') which submitted dozens to hundreds of forms a day, each containing anonymized medical data about study participants. Operations were safety-critical because changes in participants' vitals would notify safety specialists via an alarm system. Privacy was critical because these sites operated in locations where the knowledge that someone was participating in one of these trials could endanger a patient due to ignorance about HIV/AIDS.

While in this position I had about 100 local customers (data entry specialists, statisticians, primary investigators, project managers and a programming team), and around a thousand non-local customers, not including the patients, whom I never interacted with (naturally), but whom I still consider to be customers.

My duties included a major upgrade from version 3 to 4 of the software I was responsible for and the graceful handling of a storage corruption issue which cropped up in the infrastructure. The organization presented me with a certificate of appreciation after the dust settled from these projects.

The specific 'hard' technical skills I employed most heavily were fixing and extending Perl automation, learning and using the custom automation language built into the software, communicating about the technical capabilities and limitations of the software and troubleshooting fax-modem operations (it was usually line noise).

FHCRC - SCHARP

Performed a major software upgrade
Refactored code defining clinical trial forms
Adapted form code to take new security model into account
Corrected and back-filled data reporting automation
Supported in-house developers of systems extending DataFax
Recovered from infrastructure issue which caused data corruption

DataFax is the data management software used by the SCHARP organization within Fred Hutch Cancer Research Center. SCHARP stands for Statistical Center for HIV and AIDS Research and Prevention. The data in question was study participant data used by data scientists and primary investigators to execute clinical trials for medications and methods. For an example of a study which was a major part of my responsibilities, see https://www.mtnstopshiv.org/research/studies/mtn-003

In these studies, project managers directed clinical sites to sign up participants and meet with them regularly as directed under the approved protocol for that study. Depending on what was being tested, participants might take a medication, try to change a behavior, or both, and the site would take their vitals and observations when the participant came in. The data from those meetings went onto forms which the sites sent to SCHARP via FAX and email-FAX. The DataFax software was programmed to perform Optical Character Recognition on the forms, which were designed to limit errors. The Data Operations Group (referred to as DOGs) would validate and correct the invested forms, and the safety group would reach out to sites if any of the data exceeded limits which might indicate that participants might be at risk, either from participation or even other unrelated causes. Data scientists wrote automation to pull this corrected data from out of DataFax to then analyze to determine the results of the study.

My role in all this was to keep DataFax running securely, correctly, and with sufficient performance to not slow down the DOGs. In support of that, I was responsible for updates, monitoring, troubleshooting and coming up with ways to prepare for future changes and unexpected events.

When the DataFax company released a version 4 of their software, I was entirely responsible for figuring out how to move the studies from version 3 without any changes that would negatively impact their protocol conformance. To do that, I wrote a library in the DataFax scripting language which bridge the modules using the old API to the new API. The only changes required were to change each study's library to use my bridge. Explicitly requiring the studies to switch over was how I ensured that it was easy to tell whether a study had been upgraded. My library made it so that the arguments to the funciton calls didn't change, only the names. The typical edited lines per study was around five: one to include my library and four to change the names of the called functions. This made validation fast and simple.

The new version also provided an Internet-facing service to expose features to sites, which constituted a new attack surface. Our security specialist was adamant that under no circumstances would he sign off on opening ports for this. I wrote a five page paper which changed his mind by breaking down exactly what the exposure was, all the worst case scenarios, the costs of breaches (both in terms of money and more importantly, safety), and how the software and I were mitigating all those risks. The paper changed his mind.

The monitoring systems I inherited, and indeed all the automation outside of DataFax itself, were all written in Perl, which I already had extensive experience with. One such script was 7000 lines, of which at least half was long rambling comments written by someone who was sleep deprived (or so he told me). When I joined, the script had stopped working a few weeks prior, and my first task was both to fix it and also process the backlog. The script monitored incoming study data and sent reports out to sites so they could verify that we received and processed the same number of forms as they sent. I updated the script, did a code review with my manager and processed the backlog in my first week.

We didn't have many issues to troubleshoot. Occasionally sites had problems with their faxes which I diagnosed based on what came through on our end, and also on the noises I heard on the line while talking with them. Line noise was a big problem, especially in older cities. However, we did have one storage failure which caused DataFax to misbehave. As soon as I got a report from a user of what they were seeing I called up the infrastructure team, and they explained they had just turned quota monitoring on. Knowing how DataFax works, I had a good idea of how badly it would fail under these conditions, so I stopped the service, sent an announcement to users and other stakeholders, and contacted the vendor to confirm my assumptions and ask for alternative suggestions. They said my assumptions were correct and the remediation I proposed (rolling back to a snapshot from before quota monitoring) was the only safe option. I then contacted the head of the DOGs and she looped in a project manager to drive resolution. I fully documented my plan and why it was the best option and then presented it to the board the next morning for their approval. There were no questions, so after that I went back to my office and worked all day with the project manager and DOGs manager to validate the rolled-back studies and document the event for each of the studies so that they remained within data integrity guarantee requirements.

At SCHARP I learned a great deal about working within strict requirements, writing good documentation, handling surprises, working with people who have different styles, making changes as small and clear as possible, testing, and integrating diverse software (DataFax, SAS, Perl scripts, Java programs).

DISNEY

Supported engineers and content producers of Disney's web properties
Assisted in migration from internal Java servlet engines to Tomcat
Assisted in launches of the Pirates of the Caribbean online game
Wrote 'How To Be Perfect' document about data center change control
Installed cabling and servers in newly expanded data center
Supported internally developed email marketing system
Re-architected system to reduce cost and increase availability

I worked in four roles, the first and third being in the same group.

In the "Control Center" / "Service Operations Center" I responded to internal user reports and automated alarms about services malfunctioning and either solved the problem myself or contacted a specialist to address the issue. The services were mostly Java servlets running on one of two internally developed servlet engines, and later on Tomcat. Most of the services were also guest-facing websites such as espn.go.com and disney.go.com. (All sites were under go.com for cookie sharing purposes). In the time when I wasn't responding to alerts and calls, I worked on automation and integration. My second time on the team I improved the connection between our main monitoring software "SiteScope" and HP's ITIL stack, "Service Desk". I also created a single web page which combined all of the SiteScope instances' views, which made analyzing new incidents faster and easier. I also worked with another engineer to evaluate virtualization to see how much it would cost and how much it would save us (not much and a lot, respectively), to present our findings, and to get the transition started. We chose VMware as our virtualization platform.

While in the Infrastructure group I was responsible for "rack-and-stack" in the primary data center and managing the infrastructure services such as DHCP, FTP and worked with the network team and service owners to address scaling concerns. While I was on that team we expanded the main data center room by a third, installing wiring and servers and handing the servers off to owners.

In the Mobile and Messaging group I operated the "Beacon" mass-mailing software that Disney used to communicate with guests via email. During my tenure I re-architected the services which comprised the software to reduce the hardware footprint and eliminate single points of failure. I drew up diagrams in Visio to document the before-and-after of this change, wrote change documents and presented them to the change review board, and executed the changes. As I had previously been in the team which led the change review meetings, I found change review particularly entertaining.

At Disney I learned about the challenges of introducing ITIL to a system which has grown organically for a decade, managing communication during an incident, obtaining buy-in from stakeholders, operating services at scale, ways large systems can surprise us, change management, configuration management and how to balance the needs management, engineers and guests.

